{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import dataset_fonctions as dtf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read csvs\n",
    "\n",
    "train = gpd.read_file('train.geojson', index_col=0)\n",
    "test = gpd.read_file('test.geojson', index_col=0)\n",
    "train_df = train.copy()\n",
    "test_df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "train_df['change_type']=train_df['change_type'].apply(lambda x: change_type_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changement nom des colonnes pour un codage plus simple\n",
    "\n",
    "columns={'change_status_date5': 'status5','change_status_date4': 'status4', 'change_status_date3': 'status3','change_status_date2': 'status2','change_status_date1': 'status1'}\n",
    "\n",
    "train_df = train_df.rename(columns=columns)\n",
    "test_df = test_df.rename(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigne des valeurs numériques aux différents états\n",
    "\n",
    "dico = {'Prior Construction':0,'Greenland':2, 'Excavation':1,  'Land Cleared':3, 'Materials Dumped':4, 'Construction Started':5, 'Construction Midway':6, 'Construction Done':7, 'Operational':8}\n",
    "\n",
    "dtf.mapping(train_df,test_df,dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### transformer les dates de la df en format de date :\n",
    "   \n",
    "dtf.formater_dates_df(train_df)\n",
    "dtf.formater_dates_df(test_df)\n",
    "\n",
    "### on cherche ici à rajouter des colonnes correspondant au nombres de jours entre deux dates consécutives.\n",
    "\n",
    "dtf.rajout_timedelta(train_df)\n",
    "dtf.rajout_timedelta(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.dropna(subset=['status5', 'status4','status3','status2','status1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['geometry']]=train_df[['geometry']].to_crs(epsg=9834)\n",
    "test_df[['geometry']]=test_df[['geometry']].to_crs(epsg=9834)\n",
    "\n",
    "train_df['area']=train_df[['geometry']].area\n",
    "test_df['area']=test_df[['geometry']].area\n",
    "train_df['length']=train_df[['geometry']].length\n",
    "test_df['length']=test_df[['geometry']].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['1/l']=1/test_df['length']\n",
    "train_df['1/l']=1/train_df['length']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on remplace les valeurs d'aires et de surface non définie par le quantile 50%\n",
    "\n",
    "test_df = test_df.replace(np.inf, np.nan)\n",
    "\n",
    "test_df['area']=test_df['area'].fillna(test_df['area'].quantile())\n",
    "test_df['length']=test_df['length'].fillna(test_df['length'].quantile())\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "test_df[['status5','status4','status3','status2','status1','time_delta_21','time_delta_32','time_delta_43','time_delta_54','area','length','1/l']]=imputer.fit_transform(test_df[['status5','status4','status3','status2','status1','time_delta_21','time_delta_32','time_delta_43','time_delta_54','area','length','1/l']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demolition_df = train_df[train_df['change_type'] == 0]\n",
    "road_df = train_df[train_df['change_type'] == 1]\n",
    "residential_df = train_df[train_df['change_type'] == 2]\n",
    "commercial_df = train_df[train_df['change_type'] == 3]\n",
    "industrial_df = train_df[train_df['change_type'] == 4]\n",
    "mega_projects_df = train_df[train_df['change_type'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = [demolition_df,road_df,residential_df,commercial_df,industrial_df,mega_projects_df]\n",
    "\n",
    "for col in ['area','length','a/l','a/ll','sa/l']:\n",
    "    plt.figure() \n",
    "    i=0\n",
    "    for df in list_df:\n",
    "        sns.distplot(df[(df['area']<800) | (df['length']<0.1)][col],label = i)\n",
    "        i+=1\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "geometry_ppt = poly.fit_transform(train_df[['area','length','1/l']])\n",
    "geometry_ppt_test = poly.fit_transform(test_df[['area','length','1/l']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split les valeurs nominales d'urban et de geography pour pouvoir la traiter correctement avec un MultiLabelBinarizer\n",
    "    \n",
    "dtf.split_nominal(train_df,test_df)\n",
    "urban_types_mlb, urban_types_mlb_test, geography_types_mlb, geography_types_mlb_test = dtf.mlb_urban_geography(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## création des paramètres à rentrer dans l'algorithme de ML\n",
    "\n",
    "A = np.asarray(train_df[['status5','status4','status3','status2','status1','time_delta_21','time_delta_32','time_delta_43','time_delta_54']])\n",
    "train_x = np.concatenate((A,geometry_ppt,urban_types_mlb,geography_types_mlb), axis=1)\n",
    "print(train_x.shape)\n",
    "train_y = train_df['change_type']\n",
    "\n",
    "\n",
    "B = np.asarray(test_df[['status5','status4','status3','status2','status1','time_delta_21','time_delta_32','time_delta_43','time_delta_54']])\n",
    "test_x = np.concatenate((B,geometry_ppt_test,urban_types_mlb_test,geography_types_mlb_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproceseur = make_pipeline(RobustScaler(),SelectKBest(k=20))\n",
    "#DTC = DecisionTreeClassifier(max_depth=12,max_features=6)\n",
    "model = make_pipeline(RandomForestClassifier(random_state=42))\n",
    "\n",
    "\n",
    "#param_grid={'randomforestclassifier__max_depth':np.arange(6,20,4),'randomforestclassifier__max_features':np.arange(3,6),'pipeline__selectkbest__k':[5,6]}\n",
    "\n",
    "param_grid={'randomforestclassifier__n_estimators':[190],'randomforestclassifier__max_depth':[9],'randomforestclassifier__max_features':[11],'randomforestclassifier__min_samples_split':[202]}\n",
    "grid = RandomizedSearchCV(model,param_grid,cv=5,scoring='f1_micro',n_iter=1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(max_depth=5,max_features=18,min_samples_split=3860)\n",
    "\n",
    "preproceseur = make_pipeline(RobustScaler())\n",
    "model = make_pipeline(preproceseur,RandomForestClassifier(random_state=42,n_estimators=200,max_depth=9,max_features=11,min_samples_split=500))\n",
    "#model = make_pipeline(AdaBoostClassifier(random_state=42,base_estimator=DTC,n_estimators=8))\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "pred_y = model.predict(test_x)\n",
    "\n",
    "\n",
    "## Save results to submission file\n",
    "pred_df = pd.DataFrame(pred_y, columns=['change_type'])\n",
    "pred_df.to_csv(\"knn_sample_submission.csv\", index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproceseur = make_pipeline(RobustScaler())\n",
    "DTC = DecisionTreeClassifier()\n",
    "model = make_pipeline(DTC)\n",
    "\n",
    "\n",
    "\n",
    "#param_grid={'randomforestclassifier__max_depth':np.arange(6,20,4),'randomforestclassifier__max_features':np.arange(3,6),'pipeline__selectkbest__k':[5,6]}\n",
    "\n",
    "param_grid={'decisiontreeclassifier__max_depth':np.arange(1,6),'decisiontreeclassifier__max_features':np.arange(10,20),'decisiontreeclassifier__min_samples_split':np.arange(10,5000,50)}\n",
    "grid = RandomizedSearchCV(model,param_grid,cv=4,scoring='f1_micro',n_iter=50)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproceseur = make_pipeline(RobustScaler())\n",
    "DTC = DecisionTreeClassifier(max_depth=5,max_features=18,min_samples_split=3860)\n",
    "model = make_pipeline(preproceseur,GradientBoostingClassifier(random_state=42))\n",
    "\n",
    "\n",
    "#param_grid={'randomforestclassifier__max_depth':np.arange(6,20,4),'randomforestclassifier__max_features':np.arange(3,6),'pipeline__selectkbest__k':[5,6]}\n",
    "\n",
    "param_grid={'gradientboostingclassifier__n_estimators':[50,100,200],'gradientboostingclassifier__max_depth':[3,6,9],'gradientboostingclassifier__max_features':[7,6,8],'gradientboostingclassifier__min_samples_split':[10,100,500]}\n",
    "grid = RandomizedSearchCV(model,param_grid,cv=4,scoring='f1_micro',n_iter=3)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "grid.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
